{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Churn Prediction \u2013 End-to-End ML Project\n",
        "\n",
        "This notebook builds a small but complete **machine learning pipeline** for predicting whether a customer will churn.\n",
        "\n",
        "We will:\n",
        "\n",
        "1. Generate a synthetic \"telecom churn\" dataset.\n",
        "2. Explore the data with **pandas** and **matplotlib**.\n",
        "3. Train and evaluate baseline models with **scikit-learn**.\n",
        "4. Visualize performance and feature importance.\n",
        "\n",
        "You can use this as a portfolio-ready ML project on GitHub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Create a synthetic churn dataset\n",
        "\n",
        "We will simulate a simple telecom-style dataset with:\n",
        "\n",
        "- `monthly_charges`\n",
        "- `tenure_months`\n",
        "- `num_support_calls`\n",
        "- `num_addon_services`\n",
        "- `has_paperless_billing`\n",
        "- `uses_mobile_app`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1500,\n",
        "    n_features=6,\n",
        "    n_informative=4,\n",
        "    n_redundant=0,\n",
        "    n_clusters_per_class=2,\n",
        "    weights=[0.7, 0.3],  # 30% churn rate\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "# Wrap into a DataFrame with meaningful feature names\n",
        "columns = [\n",
        "    \"monthly_charges\",\n",
        "    \"tenure_months\",\n",
        "    \"num_support_calls\",\n",
        "    \"num_addon_services\",\n",
        "    \"has_paperless_billing\",\n",
        "    \"uses_mobile_app\",\n",
        "]\n",
        "df = pd.DataFrame(X, columns=columns)\n",
        "df[\"churn\"] = y\n",
        "\n",
        "# Apply some simple transformations so numbers look realistic\n",
        "df[\"monthly_charges\"] = (df[\"monthly_charges\"] - df[\"monthly_charges\"].min())\n",
        "df[\"monthly_charges\"] = 30 + 70 * df[\"monthly_charges\"] / df[\"monthly_charges\"].max()\n",
        "\n",
        "df[\"tenure_months\"] = (df[\"tenure_months\"] - df[\"tenure_months\"].min())\n",
        "df[\"tenure_months\"] = (24 * df[\"tenure_months\"] / df[\"tenure_months\"].max()).round()\n",
        "\n",
        "df[\"num_support_calls\"] = (df[\"num_support_calls\"] - df[\"num_support_calls\"].min())\n",
        "df[\"num_support_calls\"] = (5 * df[\"num_support_calls\"] / df[\"num_support_calls\"].max()).round()\n",
        "\n",
        "df[\"num_addon_services\"] = (df[\"num_addon_services\"] - df[\"num_addon_services\"].min())\n",
        "df[\"num_addon_services\"] = (4 * df[\"num_addon_services\"] / df[\"num_addon_services\"].max()).round()\n",
        "\n",
        "# Convert last two features to \"binary-ish\" flags for variety\n",
        "df[\"has_paperless_billing\"] = (df[\"has_paperless_billing\"] > df[\"has_paperless_billing\"].median()).astype(int)\n",
        "df[\"uses_mobile_app\"] = (df[\"uses_mobile_app\"] > df[\"uses_mobile_app\"].median()).astype(int)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Class balance\n",
        "class_counts = df[\"churn\"].value_counts().sort_index()\n",
        "class_counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Plot histograms for a few key numeric features\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
        "\n",
        "axes = axes.flatten()\n",
        "features_to_plot = [\"monthly_charges\", \"tenure_months\", \"num_support_calls\", \"num_addon_services\"]\n",
        "\n",
        "for ax, col in zip(axes, features_to_plot):\n",
        "    ax.hist(df[col], bins=20)\n",
        "    ax.set_title(col)\n",
        "    ax.set_xlabel(col)\n",
        "    ax.set_ylabel(\"Count\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also look at how the churn rate varies with some of these features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Helper: compute churn rate per binned feature\n",
        "def plot_binned_churn(feature, bins, figsize=(5, 3)):\n",
        "    temp = df.copy()\n",
        "    temp[\"bin\"] = pd.cut(temp[feature], bins=bins)\n",
        "    churn_rate = temp.groupby(\"bin\")[\"churn\"].mean()\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    churn_rate.plot(kind=\"bar\")\n",
        "    plt.ylabel(\"Churn rate\")\n",
        "    plt.title(f\"Churn rate vs {feature}\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_binned_churn(\"monthly_charges\", bins=6)\n",
        "plot_binned_churn(\"tenure_months\", bins=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train / Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "feature_cols = [\n",
        "    \"monthly_charges\",\n",
        "    \"tenure_months\",\n",
        "    \"num_support_calls\",\n",
        "    \"num_addon_services\",\n",
        "    \"has_paperless_billing\",\n",
        "    \"uses_mobile_app\",\n",
        "]\n",
        "X = df[feature_cols].values\n",
        "y = df[\"churn\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Baseline model \u2013 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = log_reg.predict(X_test)\n",
        "\n",
        "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "print(f\"Logistic Regression accuracy: {acc_lr:.3f}\\n\")\n",
        "print(classification_report(y_test, y_pred_lr, target_names=[\"No churn\", \"Churn\"]))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Confusion matrix plot\n",
        "cm = confusion_matrix(y_test, y_pred_lr)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4, 3))\n",
        "im = ax.imshow(cm, interpolation=\"nearest\")\n",
        "ax.set_title(\"Logistic Regression \u2013 Confusion Matrix\")\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"Actual\")\n",
        "\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_yticks([0, 1])\n",
        "ax.set_xticklabels([\"No churn\", \"Churn\"])\n",
        "ax.set_yticklabels([\"No churn\", \"Churn\"])\n",
        "\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
        "\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Tree-based model \u2013 Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    random_state=RANDOM_STATE,\n",
        "    max_depth=None,\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest accuracy: {acc_rf:.3f}\\n\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=[\"No churn\", \"Churn\"]))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Compare model performance\n",
        "models = [\"Logistic Regression\", \"Random Forest\"]\n",
        "accuracies = [acc_lr, acc_rf]\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.bar(models, accuracies)\n",
        "plt.ylim(0, 1.0)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Comparison\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Importance\n",
        "\n",
        "For the Random Forest model, we can inspect which features contribute most to the churn prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "importances = rf.feature_importances_\n",
        "idx = np.argsort(importances)[::-1]\n",
        "\n",
        "sorted_features = [feature_cols[i] for i in idx]\n",
        "sorted_importances = importances[idx]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(range(len(sorted_features)), sorted_importances)\n",
        "plt.xticks(range(len(sorted_features)), sorted_features, rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.title(\"Random Forest \u2013 Feature Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Wrap-up\n",
        "\n",
        "In this notebook, we:\n",
        "\n",
        "- Built a synthetic **customer churn** dataset.\n",
        "- Performed quick **EDA** with summary stats and plots.\n",
        "- Trained and evaluated **Logistic Regression** and **Random Forest** models.\n",
        "- Visualized performance and **feature importance**.\n",
        "\n",
        "This structure mirrors a typical small data science workflow and is ideal for a GitHub portfolio project."
      ]
    }
  ]
}